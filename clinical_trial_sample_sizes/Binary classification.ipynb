{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used python3 to have l1_ratio option for logistic regression from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, pickle, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_df_filter2 = pd.read_csv('PD_outcomes_filter2yrs.csv')\n",
    "surv_df_filter3 = pd.read_csv('PD_outcomes_filter3yrs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../finalized_outcome_survival_models/final_all_covariate_sets.pkl', 'rb') as f:\n",
    "    all_covariate_sets = pickle.load(f, encoding='latin1')\n",
    "baseline_df = pd.read_csv('../finalized_outcome_survival_models/final_survival_baseline_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_patnos_filter2.pkl', 'rb') as f:\n",
    "    filter2_test_patnos = pickle.load(f, encoding='latin1')\n",
    "with open('test_patnos_filter3.pkl', 'rb') as f:\n",
    "    filter3_test_patnos = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2047)\n",
    "motor_surv_df_filter2 = surv_df_filter2[['PATNO','Motor_T','Motor_E']]\n",
    "motor_bl_cov_set = list(all_covariate_sets['baseline']['Motor'])\n",
    "motor_bl_df = baseline_df[['PATNO'] + motor_bl_cov_set]\n",
    "motor_bl_df = motor_surv_df_filter2.merge(motor_bl_df, on=['PATNO'], validate='one_to_one').dropna()\n",
    "assert set(filter2_test_patnos['Motor'].tolist()).issubset(set(motor_bl_df.PATNO.values.tolist()))\n",
    "test_motor_bl_df = motor_bl_df.loc[motor_bl_df['PATNO'].isin(filter2_test_patnos['Motor'])]\n",
    "train_valid_motor_bl_df = motor_bl_df.loc[~motor_bl_df['PATNO'].isin(filter2_test_patnos['Motor'])]\n",
    "test_X = test_motor_bl_df[motor_bl_cov_set].values\n",
    "test_Y = test_motor_bl_df['Motor_E'].values\n",
    "test_patnos_arr = test_motor_bl_df['PATNO'].values\n",
    "train_valid_patnos = train_valid_motor_bl_df.PATNO.values\n",
    "np.random.shuffle(train_valid_patnos)\n",
    "train_X_folds = []\n",
    "train_Y_folds = []\n",
    "valid_X_folds = []\n",
    "valid_Y_folds = []\n",
    "for fold_idx in range(4):\n",
    "    valid_patnos \\\n",
    "        = train_valid_patnos[int(fold_idx/4.*len(train_valid_patnos)):int((fold_idx+1)/4.*len(train_valid_patnos))]\n",
    "    train_patnos = set(train_valid_patnos.tolist()).difference(set(valid_patnos.tolist()))\n",
    "    train_motor_bl_df = train_valid_motor_bl_df.loc[train_valid_motor_bl_df['PATNO'].isin(train_patnos)]\n",
    "    valid_motor_bl_df = train_valid_motor_bl_df.loc[train_valid_motor_bl_df['PATNO'].isin(valid_patnos)]\n",
    "    train_X_folds.append(train_motor_bl_df[motor_bl_cov_set].values)\n",
    "    train_Y_folds.append(train_motor_bl_df['Motor_E'].values)\n",
    "    valid_X_folds.append(valid_motor_bl_df[motor_bl_cov_set].values)\n",
    "    valid_Y_folds.append(valid_motor_bl_df['Motor_E'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "fold_test_metrics_dict = {'auroc': [], 'acc': [], 'prec': [], 'rec': []}\n",
    "fold_test_coefs = pd.DataFrame({'Feature': motor_bl_cov_set})\n",
    "fold_vary_l1_ratio_fig, fold_vary_l1_ratio_ax = plt.subplots(nrows=4, ncols=4, figsize=(20,20), sharex=True, \\\n",
    "                                                             sharey=True)\n",
    "fold_vary_C_fig, fold_vary_C_ax = plt.subplots(nrows=4, ncols=4, figsize=(20,20), sharex=True, sharey=True)\n",
    "metric_list = ['auroc', 'acc', 'prec', 'rec']\n",
    "metric_human_readable_list = ['AUROC', 'Accuracy', 'Precision', 'Recall']\n",
    "for fold_idx in range(4):\n",
    "    train_X = train_X_folds[fold_idx]\n",
    "    train_Y = train_Y_folds[fold_idx]\n",
    "    valid_X = valid_X_folds[fold_idx]\n",
    "    valid_Y = valid_Y_folds[fold_idx]\n",
    "    l1_ratios = [0., 0.5, 1.0]\n",
    "    Cs = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1., 5., 10., 50., 100., 500., 1000., 5000., 1e4]\n",
    "    best_valid_auroc = 0.\n",
    "    best_valid_acc = 0.\n",
    "    best_valid_prec = 0.\n",
    "    best_valid_rec = 0.\n",
    "    train_vary_l1_ratio_metrics_dict = dict()\n",
    "    train_vary_C_metrics_dict = dict()\n",
    "    valid_vary_l1_ratio_metrics_dict = dict()\n",
    "    valid_vary_C_metrics_dict = dict()\n",
    "    for l1_ratio in l1_ratios:\n",
    "        train_vary_C_metrics_dict[l1_ratio] = {'auroc': [], 'acc': [], 'prec': [], 'rec': []}\n",
    "        valid_vary_C_metrics_dict[l1_ratio] = {'auroc': [], 'acc': [], 'prec': [], 'rec': []}\n",
    "    for C in Cs:\n",
    "        train_vary_l1_ratio_metrics_dict[C] = {'auroc': [], 'acc': [], 'prec': [], 'rec': []}\n",
    "        valid_vary_l1_ratio_metrics_dict[C] = {'auroc': [], 'acc': [], 'prec': [], 'rec': []}\n",
    "        for l1_ratio in l1_ratios:\n",
    "            log_reg = LogisticRegression(penalty='elasticnet', l1_ratio=l1_ratio, C=C, max_iter=1000, solver='saga')\n",
    "            log_reg.fit(train_X, train_Y)\n",
    "            train_pred = log_reg.predict(train_X)\n",
    "            train_prob = log_reg.predict_proba(train_X)[:,1]\n",
    "            valid_pred = log_reg.predict(valid_X)\n",
    "            valid_prob = log_reg.predict_proba(valid_X)[:,1]\n",
    "            valid_auroc = roc_auc_score(valid_Y, valid_prob)\n",
    "            valid_acc = accuracy_score(valid_Y, valid_pred)\n",
    "            valid_prec = precision_score(valid_Y, valid_pred)\n",
    "            valid_rec = recall_score(valid_Y, valid_pred)\n",
    "            train_auroc = roc_auc_score(train_Y, train_prob)\n",
    "            train_acc = accuracy_score(train_Y, train_pred)\n",
    "            train_prec = precision_score(train_Y, train_pred)\n",
    "            train_rec = recall_score(train_Y, train_pred)\n",
    "            train_vary_l1_ratio_metrics_dict[C]['auroc'].append(train_auroc)\n",
    "            train_vary_l1_ratio_metrics_dict[C]['acc'].append(train_acc)\n",
    "            train_vary_l1_ratio_metrics_dict[C]['prec'].append(train_prec)\n",
    "            train_vary_l1_ratio_metrics_dict[C]['rec'].append(train_rec)\n",
    "            train_vary_C_metrics_dict[l1_ratio]['auroc'].append(train_auroc)\n",
    "            train_vary_C_metrics_dict[l1_ratio]['acc'].append(train_acc)\n",
    "            train_vary_C_metrics_dict[l1_ratio]['prec'].append(train_prec)\n",
    "            train_vary_C_metrics_dict[l1_ratio]['rec'].append(train_rec)\n",
    "            valid_vary_l1_ratio_metrics_dict[C]['auroc'].append(valid_auroc)\n",
    "            valid_vary_l1_ratio_metrics_dict[C]['acc'].append(valid_acc)\n",
    "            valid_vary_l1_ratio_metrics_dict[C]['prec'].append(valid_prec)\n",
    "            valid_vary_l1_ratio_metrics_dict[C]['rec'].append(valid_rec)\n",
    "            valid_vary_C_metrics_dict[l1_ratio]['auroc'].append(valid_auroc)\n",
    "            valid_vary_C_metrics_dict[l1_ratio]['acc'].append(valid_acc)\n",
    "            valid_vary_C_metrics_dict[l1_ratio]['prec'].append(valid_prec)\n",
    "            valid_vary_C_metrics_dict[l1_ratio]['rec'].append(valid_rec)\n",
    "            if valid_auroc > best_valid_auroc or (valid_auroc == best_valid_auroc and valid_acc > best_valid_acc) \\\n",
    "                or (valid_auroc == best_valid_auroc and valid_acc == best_valid_acc and valid_prec > best_valid_prec) \\\n",
    "                or (valid_auroc == best_valid_auroc and valid_acc == best_valid_acc and valid_prec == best_valid_prec \\\n",
    "                    and valid_rec > best_valid_rec):\n",
    "                best_valid_auroc = valid_auroc\n",
    "                best_valid_acc = valid_acc\n",
    "                best_valid_prec = valid_prec\n",
    "                best_valid_rec = valid_rec\n",
    "                best_train_metrics_list = [train_auroc, train_acc, train_prec, train_rec]\n",
    "                best_l1_ratio = l1_ratio\n",
    "                best_C = C\n",
    "                best_test_pred = log_reg.predict(test_X)\n",
    "                best_test_prob = log_reg.predict_proba(test_X)[:,1]\n",
    "                best_coefs = log_reg.coef_.flatten()\n",
    "    best_valid_metrics_list = [best_valid_auroc, best_valid_acc, best_valid_prec, best_valid_rec]\n",
    "    for metric_idx in range(len(metric_list)):\n",
    "        metric = metric_list[metric_idx]\n",
    "        fold_vary_l1_ratio_ax[metric_idx, fold_idx].plot(l1_ratios, train_vary_l1_ratio_metrics_dict[best_C][metric], \\\n",
    "                                                         c='b', linestyle='--', label='train')\n",
    "        fold_vary_l1_ratio_ax[metric_idx, fold_idx].scatter(best_l1_ratio, best_train_metrics_list[metric_idx], \\\n",
    "                                                            c='b')\n",
    "        fold_vary_l1_ratio_ax[metric_idx, fold_idx].plot(l1_ratios, valid_vary_l1_ratio_metrics_dict[best_C][metric], \\\n",
    "                                                         c='r', label='valid')\n",
    "        fold_vary_l1_ratio_ax[metric_idx, fold_idx].scatter(best_l1_ratio, best_valid_metrics_list[metric_idx], \\\n",
    "                                                            c='r')\n",
    "        fold_vary_l1_ratio_ax[metric_idx, fold_idx].set_xlabel('L1 ratio')\n",
    "        fold_vary_l1_ratio_ax[metric_idx, fold_idx].set_ylabel(metric_human_readable_list[metric_idx])\n",
    "        fold_vary_l1_ratio_ax[metric_idx, fold_idx].set_title('Fold ' + str(fold_idx))\n",
    "        fold_vary_C_ax[metric_idx, fold_idx].plot(Cs, train_vary_C_metrics_dict[best_l1_ratio][metric], c='b', \\\n",
    "                                                  linestyle='--', label='train')\n",
    "        fold_vary_C_ax[metric_idx, fold_idx].scatter(best_C, best_train_metrics_list[metric_idx], c='b')\n",
    "        fold_vary_C_ax[metric_idx, fold_idx].plot(Cs, valid_vary_C_metrics_dict[best_l1_ratio][metric], c='r', \\\n",
    "                                                  label='valid')\n",
    "        fold_vary_C_ax[metric_idx, fold_idx].scatter(best_C, best_valid_metrics_list[metric_idx], c='r')\n",
    "        fold_vary_C_ax[metric_idx, fold_idx].set_xlabel('C')\n",
    "        fold_vary_C_ax[metric_idx, fold_idx].set_ylabel(metric_human_readable_list[metric_idx])\n",
    "        fold_vary_C_ax[metric_idx, fold_idx].set_title('Fold ' + str(fold_idx))\n",
    "        fold_vary_C_ax[metric_idx, fold_idx].set_xscale('log')\n",
    "    fold_test_coefs['coef_fold' + str(fold_idx)] = best_coefs\n",
    "    fold_test_metrics_dict['auroc'].append(roc_auc_score(test_Y, best_test_prob))\n",
    "    fold_test_metrics_dict['acc'].append(accuracy_score(test_Y, best_test_pred))\n",
    "    fold_test_metrics_dict['prec'].append(precision_score(test_Y, best_test_pred))\n",
    "    fold_test_metrics_dict['rec'].append(recall_score(test_Y, best_test_pred))\n",
    "for metric_idx in range(len(metric_list)):\n",
    "    fold_vary_l1_ratio_ax[metric_idx, 3].legend()\n",
    "    fold_vary_C_ax[metric_idx, 3].legend()\n",
    "fold_vary_l1_ratio_fig.tight_layout()\n",
    "fold_vary_l1_ratio_fig.show()\n",
    "fold_vary_C_fig.tight_layout()\n",
    "fold_vary_C_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fold_test_metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_l1_ratio)\n",
    "print(best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "fold_test_metrics_dict = {'auroc': [], 'acc': [], 'prec': [], 'rec': []}\n",
    "fold_test_coefs = pd.DataFrame({'Feature': motor_bl_cov_set})\n",
    "fold_vary_n_estimators_fig, fold_vary_n_estimators_ax = plt.subplots(nrows=4, ncols=4, figsize=(20,20), sharex=True, \\\n",
    "                                                                     sharey=True)\n",
    "fold_vary_min_impurity_fig, fold_vary_min_impurity_ax \\\n",
    "    = plt.subplots(nrows=4, ncols=4, figsize=(20,20), sharex=True, sharey=True)\n",
    "metric_list = ['auroc', 'acc', 'prec', 'rec']\n",
    "metric_human_readable_list = ['AUROC', 'Accuracy', 'Precision', 'Recall']\n",
    "fold_test_preds = pd.DataFrame({'PATNO': test_patnos_arr})\n",
    "fold_test_probs = pd.DataFrame({'PATNO': test_patnos_arr})\n",
    "for fold_idx in range(4):\n",
    "    train_X = train_X_folds[fold_idx]\n",
    "    train_Y = train_Y_folds[fold_idx]\n",
    "    valid_X = valid_X_folds[fold_idx]\n",
    "    valid_Y = valid_Y_folds[fold_idx]\n",
    "    n_estimators_choices = [1, 5, 10, 20, 30, 50, 75, 100, 150, 200, 250, 300, 400, 500]\n",
    "    min_impurities = [0., 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.]\n",
    "    best_valid_auroc = 0.\n",
    "    best_valid_acc = 0.\n",
    "    best_valid_prec = 0.\n",
    "    best_valid_rec = 0.\n",
    "    train_vary_n_estimators_metrics_dict = dict()\n",
    "    train_vary_min_impurity_metrics_dict = dict()\n",
    "    valid_vary_n_estimators_metrics_dict = dict()\n",
    "    valid_vary_min_impurity_metrics_dict = dict()\n",
    "    for min_impurity in min_impurities:\n",
    "        train_vary_n_estimators_metrics_dict[min_impurity] = {'auroc': [], 'acc': [], 'prec': [], 'rec': []}\n",
    "        valid_vary_n_estimators_metrics_dict[min_impurity] = {'auroc': [], 'acc': [], 'prec': [], 'rec': []}\n",
    "    for n_estimators in n_estimators_choices:\n",
    "        train_vary_min_impurity_metrics_dict[n_estimators] = {'auroc': [], 'acc': [], 'prec': [], 'rec': []}\n",
    "        valid_vary_min_impurity_metrics_dict[n_estimators] = {'auroc': [], 'acc': [], 'prec': [], 'rec': []}\n",
    "        for min_impurity in min_impurities:\n",
    "            random_forest = RandomForestClassifier(n_estimators=n_estimators, min_impurity_decrease=min_impurity)\n",
    "            random_forest.fit(train_X, train_Y)\n",
    "            train_pred = random_forest.predict(train_X)\n",
    "            train_prob = random_forest.predict_proba(train_X)[:,1]\n",
    "            valid_pred = random_forest.predict(valid_X)\n",
    "            valid_prob = random_forest.predict_proba(valid_X)[:,1]\n",
    "            valid_auroc = roc_auc_score(valid_Y, valid_prob)\n",
    "            valid_acc = accuracy_score(valid_Y, valid_pred)\n",
    "            valid_prec = precision_score(valid_Y, valid_pred)\n",
    "            valid_rec = recall_score(valid_Y, valid_pred)\n",
    "            train_auroc = roc_auc_score(train_Y, train_prob)\n",
    "            train_acc = accuracy_score(train_Y, train_pred)\n",
    "            train_prec = precision_score(train_Y, train_pred)\n",
    "            train_rec = recall_score(train_Y, train_pred)\n",
    "            train_vary_n_estimators_metrics_dict[min_impurity]['auroc'].append(train_auroc)\n",
    "            train_vary_n_estimators_metrics_dict[min_impurity]['acc'].append(train_acc)\n",
    "            train_vary_n_estimators_metrics_dict[min_impurity]['prec'].append(train_prec)\n",
    "            train_vary_n_estimators_metrics_dict[min_impurity]['rec'].append(train_rec)\n",
    "            train_vary_min_impurity_metrics_dict[n_estimators]['auroc'].append(train_auroc)\n",
    "            train_vary_min_impurity_metrics_dict[n_estimators]['acc'].append(train_acc)\n",
    "            train_vary_min_impurity_metrics_dict[n_estimators]['prec'].append(train_prec)\n",
    "            train_vary_min_impurity_metrics_dict[n_estimators]['rec'].append(train_rec)\n",
    "            valid_vary_n_estimators_metrics_dict[min_impurity]['auroc'].append(valid_auroc)\n",
    "            valid_vary_n_estimators_metrics_dict[min_impurity]['acc'].append(valid_acc)\n",
    "            valid_vary_n_estimators_metrics_dict[min_impurity]['prec'].append(valid_prec)\n",
    "            valid_vary_n_estimators_metrics_dict[min_impurity]['rec'].append(valid_rec)\n",
    "            valid_vary_min_impurity_metrics_dict[n_estimators]['auroc'].append(valid_auroc)\n",
    "            valid_vary_min_impurity_metrics_dict[n_estimators]['acc'].append(valid_acc)\n",
    "            valid_vary_min_impurity_metrics_dict[n_estimators]['prec'].append(valid_prec)\n",
    "            valid_vary_min_impurity_metrics_dict[n_estimators]['rec'].append(valid_rec)\n",
    "            if valid_auroc > best_valid_auroc or (valid_auroc == best_valid_auroc and valid_acc > best_valid_acc) \\\n",
    "                or (valid_auroc == best_valid_auroc and valid_acc == best_valid_acc and valid_prec > best_valid_prec) \\\n",
    "                or (valid_auroc == best_valid_auroc and valid_acc == best_valid_acc and valid_prec == best_valid_prec \\\n",
    "                    and valid_rec > best_valid_rec):\n",
    "                best_valid_auroc = valid_auroc\n",
    "                best_valid_acc = valid_acc\n",
    "                best_valid_prec = valid_prec\n",
    "                best_valid_rec = valid_rec\n",
    "                best_train_metrics_list = [train_auroc, train_acc, train_prec, train_rec]\n",
    "                best_min_impurity = min_impurity\n",
    "                best_n_estimators = n_estimators\n",
    "                best_test_pred = random_forest.predict(test_X)\n",
    "                best_test_prob = random_forest.predict_proba(test_X)[:,1]\n",
    "                best_coefs = random_forest.feature_importances_.flatten()\n",
    "    best_valid_metrics_list = [best_valid_auroc, best_valid_acc, best_valid_prec, best_valid_rec]\n",
    "    for metric_idx in range(len(metric_list)):\n",
    "        metric = metric_list[metric_idx]\n",
    "        n_estim_ax = fold_vary_n_estimators_ax[metric_idx, fold_idx]\n",
    "        n_estim_ax.plot(n_estimators_choices, train_vary_n_estimators_metrics_dict[best_min_impurity][metric], c='b', \\\n",
    "                        linestyle='--', label='train')\n",
    "        n_estim_ax.scatter(best_n_estimators, best_train_metrics_list[metric_idx], c='b')\n",
    "        n_estim_ax.plot(n_estimators_choices, valid_vary_n_estimators_metrics_dict[best_min_impurity][metric], c='r', \\\n",
    "                        label='valid')\n",
    "        n_estim_ax.scatter(best_n_estimators, best_valid_metrics_list[metric_idx], c='r')\n",
    "        n_estim_ax.set_xlabel('# of estimators')\n",
    "        n_estim_ax.set_ylabel(metric_human_readable_list[metric_idx])\n",
    "        n_estim_ax.set_title('Fold ' + str(fold_idx))\n",
    "        min_impur_ax = fold_vary_min_impurity_ax[metric_idx, fold_idx]\n",
    "        min_impur_ax.plot(min_impurities, train_vary_min_impurity_metrics_dict[best_n_estimators][metric], c='b', \\\n",
    "                          linestyle='--', label='train')\n",
    "        min_impur_ax.scatter(best_min_impurity, best_train_metrics_list[metric_idx], c='b')\n",
    "        min_impur_ax.plot(min_impurities, valid_vary_min_impurity_metrics_dict[best_n_estimators][metric], c='r', \\\n",
    "                          label='valid')\n",
    "        min_impur_ax.scatter(best_min_impurity, best_valid_metrics_list[metric_idx], c='r')\n",
    "        min_impur_ax.set_xlabel('Minimum impurity decrease')\n",
    "        min_impur_ax.set_ylabel(metric_human_readable_list[metric_idx])\n",
    "        min_impur_ax.set_title('Fold ' + str(fold_idx))\n",
    "        min_impur_ax.set_xscale('log')\n",
    "    fold_test_coefs['coef_fold' + str(fold_idx)] = best_coefs\n",
    "    fold_test_metrics_dict['auroc'].append(roc_auc_score(test_Y, best_test_prob))\n",
    "    fold_test_metrics_dict['acc'].append(accuracy_score(test_Y, best_test_pred))\n",
    "    fold_test_metrics_dict['prec'].append(precision_score(test_Y, best_test_pred))\n",
    "    fold_test_metrics_dict['rec'].append(recall_score(test_Y, best_test_pred))\n",
    "    fold_test_preds['pred_fold' + str(fold_idx)] = best_test_pred\n",
    "    fold_test_probs['prob_fold' + str(fold_idx)] = best_test_prob\n",
    "for metric_idx in range(len(metric_list)):\n",
    "    fold_vary_n_estimators_ax[metric_idx, 3].legend()\n",
    "    fold_vary_min_impurity_ax[metric_idx, 3].legend()\n",
    "fold_vary_n_estimators_fig.tight_layout()\n",
    "fold_vary_n_estimators_fig.show()\n",
    "fold_vary_min_impurity_fig.tight_layout()\n",
    "fold_vary_min_impurity_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fold_test_metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check David's claim that AUROC is only ranking\n",
    "y_true = [0,0,0,1,1,1,1]\n",
    "y_pred1 = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "y_pred2 = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "y_pred3 = [0.45, 0.47, 0.49, 0.51, 0.53, 0.55, 0.57]\n",
    "y_pred4 = [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97]\n",
    "print(roc_auc_score(y_true, y_pred1))\n",
    "print(roc_auc_score(y_true, y_pred2))\n",
    "print(roc_auc_score(y_true, y_pred3))\n",
    "print(roc_auc_score(y_true, y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check David's claim that AUROC is only ranking\n",
    "y_true = [0,1,0,1,1,0,1]\n",
    "y_pred1 = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "y_pred2 = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "y_pred3 = [0.45, 0.47, 0.49, 0.51, 0.53, 0.55, 0.57]\n",
    "y_pred4 = [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97]\n",
    "print(roc_auc_score(y_true, y_pred1))\n",
    "print(roc_auc_score(y_true, y_pred2))\n",
    "print(roc_auc_score(y_true, y_pred3))\n",
    "print(roc_auc_score(y_true, y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def calc_sample_size(p_c, relative_risk_reduction, power):\n",
    "    p_t = (1.-relative_risk_reduction)*p_c\n",
    "    a = norm.ppf(0.975) # assume alpha = 0.05\n",
    "    b = norm.ppf(power)\n",
    "    return (a+b)**2*(p_c*(1.-p_c) + p_t*(1.-p_t))/((p_c-p_t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sample size reduction from pred\n",
    "import math\n",
    "inc_test_patnos = fold_test_preds.loc[fold_test_preds['pred_fold0']==1].PATNO.values\n",
    "orig_test_patnos = fold_test_preds['PATNO'].values\n",
    "inc_p_c = surv_df_filter2.loc[surv_df_filter2['PATNO'].isin(inc_test_patnos)]['Motor_E'].mean()\n",
    "orig_p_c = surv_df_filter2.loc[surv_df_filter2['PATNO'].isin(orig_test_patnos)]['Motor_E'].mean()\n",
    "inc_n = int(math.ceil(calc_sample_size(inc_p_c, .2, .8)))\n",
    "orig_n = int(math.ceil(calc_sample_size(orig_p_c, .2, .8)))\n",
    "inc_screen_n = int(math.ceil(inc_n*len(orig_test_patnos)/float(len(inc_test_patnos))))\n",
    "print('{0:.2f}%'.format(inc_p_c*100.) + ' of ' + str(len(inc_test_patnos)) + ' predicted patients observed')\n",
    "print('{0:.2f}%'.format(orig_p_c*100.) + ' of ' + str(len(orig_test_patnos)) + ' test patients observed')\n",
    "print('Required sample size reduced from ' + str(orig_n) + ' to ' + str(inc_n))\n",
    "print('Anticipate screening ' + str(inc_screen_n) + ' patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_test_patnos = fold_test_preds.loc[fold_test_preds['pred_fold1']==1].PATNO.values\n",
    "orig_test_patnos = fold_test_preds['PATNO'].values\n",
    "print(len(inc_test_patnos))\n",
    "inc_p_c = surv_df_filter2.loc[surv_df_filter2['PATNO'].isin(inc_test_patnos)]['Motor_E'].mean()\n",
    "orig_p_c = surv_df_filter2.loc[surv_df_filter2['PATNO'].isin(orig_test_patnos)]['Motor_E'].mean()\n",
    "print(calc_sample_size(inc_p_c, .2, .8))\n",
    "print(calc_sample_size(orig_p_c, .2, .8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_test_patnos = fold_test_preds.loc[fold_test_preds['pred_fold2']==1].PATNO.values\n",
    "orig_test_patnos = fold_test_preds['PATNO'].values\n",
    "print(len(inc_test_patnos))\n",
    "inc_p_c = surv_df_filter2.loc[surv_df_filter2['PATNO'].isin(inc_test_patnos)]['Motor_E'].mean()\n",
    "orig_p_c = surv_df_filter2.loc[surv_df_filter2['PATNO'].isin(orig_test_patnos)]['Motor_E'].mean()\n",
    "print(calc_sample_size(inc_p_c, .2, .8))\n",
    "print(calc_sample_size(orig_p_c, .2, .8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_test_patnos = fold_test_preds.loc[fold_test_preds['pred_fold3']==1].PATNO.values\n",
    "orig_test_patnos = fold_test_preds['PATNO'].values\n",
    "print(len(inc_test_patnos))\n",
    "inc_p_c = surv_df_filter2.loc[surv_df_filter2['PATNO'].isin(inc_test_patnos)]['Motor_E'].mean()\n",
    "orig_p_c = surv_df_filter2.loc[surv_df_filter2['PATNO'].isin(orig_test_patnos)]['Motor_E'].mean()\n",
    "print(calc_sample_size(inc_p_c, .2, .8))\n",
    "print(calc_sample_size(orig_p_c, .2, .8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
